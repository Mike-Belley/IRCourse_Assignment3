{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "K0DYVialinFH",
        "Gu817af8isGj",
        "x3agWuAEwUOn",
        "bh_uoIg-82rj"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "K0DYVialinFH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFIBZsG3ieb4",
        "outputId": "45b28f34-a641-41c6-ce64-81edd72e6b3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import string\n",
        "\n",
        "import csv\n",
        "\n",
        "import math\n",
        "\n",
        "from post_parser_record import PostParserRecord\n",
        "post_reader = PostParserRecord(\"Posts_Coffee.xml\")\n",
        "\n",
        "from scipy.spatial import distance"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF"
      ],
      "metadata": {
        "id": "Gu817af8isGj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF:\n",
        "\n",
        "TF = term frequency in a document / total number of words in the document\n",
        "\n",
        "Will be stored in a dictionary like an inverted index\n",
        "\n",
        "IDF = log( total number of docs / number of docs with the term t )\n",
        "\n",
        "Will be stored in a dictionary with {term: IDF, term2: IDF2...} Each term will have 1 IDF score. Can use the TF dictionary to see how many docs each term is in\n",
        "\n",
        "After you have these two dictionaries, go thru the TF one, multiply each value by the corresponding IDF score, and store this in a new dictionary"
      ],
      "metadata": {
        "id": "ySxQzMvei_zX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF:\n",
        "\n",
        "tokenize, remove stop words and such\n",
        "\n",
        "go thru all words\n",
        "\n",
        "  if the word is not in the tf dictionary then initalize it with {doc_id: 1/total terms}\n",
        "\n",
        "  else if the word is in the dict, but doc_id isnt in the word then initalize the doc_id with 1/total terms\n",
        "\n",
        "  else (the word and doc_id is in the dict) then increment by 1/total terms\n",
        "\n",
        "\n",
        "IDF: \n",
        "\n",
        "find total number of docs\n",
        "\n",
        "go thru all of the words in the tf dictionary\n",
        "\n",
        "  IDF[word] = log2( # of docs / len(tf_dict[word].keys()) )\n",
        "\n",
        "\n",
        "TF-IDF:\n",
        "\n",
        "go thru all the words in tf\n",
        "\n",
        "go thru all the doc_ids for the word\n",
        "\n",
        "  if word not in tf_idf.keys() then initialize, tf_idf[word] = {doc_id: tf[word][doc_id] * idf[word]\n",
        "\n",
        "  else (word is in tf_idf) tf_idf[word][doc_id] = tf[word][doc_id] * idf[word]"
      ],
      "metadata": {
        "id": "OC5Y5NTo3Sx1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_tf_idf():\n",
        "\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  stop_words.add('p')\n",
        "\n",
        "  # initialize tf, idf, and tf-idf dictionaries\n",
        "  tf_dict = {}\n",
        "  idf_dict = {}\n",
        "  tf_idf_dict = {}\n",
        "\n",
        "  # calculate tf\n",
        "  # go thru all questions\n",
        "  for question_id in post_reader.map_questions:\n",
        "    question = post_reader.map_questions[question_id]\n",
        "\n",
        "    # tokenize the title and body and filter out the unwanted stuff\n",
        "    word_tokens = word_tokenize(question.title)\n",
        "    word_tokens += word_tokenize(question.body)\n",
        "    filtered_text = [word.lower() for word in word_tokens if word not in stop_words and word.isalnum()]\n",
        "\n",
        "    # get the amount of terms in the doc\n",
        "    terms_amt = len(filtered_text)\n",
        "\n",
        "    # go thru all the strings in filtered_text\n",
        "    for word in filtered_text:\n",
        "      # if the word is already in the dictionary\n",
        "      if word in tf_dict.keys():\n",
        "        # if the questionid for the word is there, increment\n",
        "        if question_id in tf_dict[word].keys():\n",
        "          tf_dict[word][question_id] += (1 / terms_amt)\n",
        "\n",
        "        # if the questionid for the word is not there, initialize\n",
        "        else:\n",
        "          tf_dict[word][question_id] = (1 / terms_amt)\n",
        "\n",
        "      # if the word is not in the dictionary, initialize it\n",
        "      else:\n",
        "        tf_dict[word] = {question_id: (1 / terms_amt)}\n",
        "\n",
        "  # go thru all the answers\n",
        "  for answer_id in post_reader.map_just_answers:\n",
        "    answer = post_reader.map_just_answers[answer_id]\n",
        "\n",
        "    # tokenize the body and filter out the unwanted stuff\n",
        "    word_tokens = word_tokenize(answer.body)\n",
        "    filtered_text = [word.lower() for word in word_tokens if word not in stop_words and word.isalnum()]\n",
        "\n",
        "    # get the amount of terms in the doc\n",
        "    terms_amt = len(filtered_text)\n",
        "\n",
        "    # go thru all the strings in filtered_text\n",
        "    for word in filtered_text:\n",
        "      # if the word is already in the dictionary\n",
        "      if word in tf_dict.keys():\n",
        "        # if the answerid for the word is there, increment\n",
        "        if answer_id in tf_dict[word].keys():\n",
        "          tf_dict[word][answer_id] += (1 / terms_amt)\n",
        "\n",
        "        # if the answerid for the word is not there, initialize\n",
        "        else:\n",
        "          tf_dict[word][answer_id] = (1 / terms_amt)\n",
        "\n",
        "      # if the word is not in the dictionary, initialize it\n",
        "      else:\n",
        "        tf_dict[word] = {answer_id: (1 / terms_amt)}\n",
        "    \n",
        "  \n",
        "  # calculate idf\n",
        "  # get total amount of docs\n",
        "  doc_amount = len(post_reader.map_questions) + len(post_reader.map_just_answers)\n",
        "\n",
        "  # go thru all the words in tf dictionary and calculate idf\n",
        "  idf_dict = {word: (math.log( (doc_amount / len(tf_dict[word].keys())) , 2)) for word in tf_dict.keys()}\n",
        "\n",
        "\n",
        "  # calculate tf-idf\n",
        "  # go thru all the words in tf dictionary\n",
        "  for word in tf_dict.keys():\n",
        "    # go thru all the docs for that word\n",
        "    for doc_id in tf_dict[word].keys():\n",
        "      # calculate tf-idf and store it in tf-idf dictionary\n",
        "      # if word is not already in tf-idf dict then initialize\n",
        "      if word not in tf_idf_dict.keys():\n",
        "        tf_idf_dict[word] = {doc_id: (tf_dict[word][doc_id] * idf_dict[word])}\n",
        "      # if word is already in tf-idf dict then add the docid\n",
        "      else:\n",
        "        tf_idf_dict[word][doc_id] = (tf_dict[word][doc_id] * idf_dict[word])\n",
        "  \n",
        "  # return the tf-idf dictionary\n",
        "  return tf_idf_dict\n",
        "\n",
        "# call the tf-idf processing function and store results in a dictionary\n",
        "tf_idf_dict = process_tf_idf()"
      ],
      "metadata": {
        "id": "WkHHn3TpiuGh"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ranking based on TF-IDF:\n",
        "\n",
        "Term at a time"
      ],
      "metadata": {
        "id": "UuPjS7fkwlyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a search and print the top results\n",
        "def tf_idf_search(search_text):\n",
        "\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  stop_words.add('p')\n",
        "\n",
        "  # tokenize the body and filter out the unwanted stuff\n",
        "  word_tokens = word_tokenize(search_text)\n",
        "  search_words = [word.lower() for word in word_tokens if word not in stop_words and word.isalnum()]\n",
        "\n",
        "  # initialize a dictionary to the inverted index entry of the first word in the search\n",
        "  search_docs_rank = tf_idf_dict[search_words[0]].copy()\n",
        "\n",
        "  # print(search_words)\n",
        "\n",
        "  # go thru the rest of the terms\n",
        "  for i in range(1, len(search_words)):\n",
        "    # put the scores of the word in a temp dictionary\n",
        "    temp_dict = tf_idf_dict[search_words[i]].copy()\n",
        "\n",
        "    # go thru the doc_ids in the temp dictionary\n",
        "    for doc_id in temp_dict:\n",
        "\n",
        "      # if the doc_id is already in the main dict, then increment\n",
        "      if doc_id in search_docs_rank.keys():\n",
        "        search_docs_rank[doc_id] += temp_dict[doc_id]\n",
        "\n",
        "      # if the doc_id is not in the main dict, initialize\n",
        "      else:\n",
        "        search_docs_rank[doc_id] = temp_dict[doc_id]\n",
        " \n",
        "  # print(search_docs_rank)\n",
        "  # order by the values\n",
        "  sorted_by_value = dict(sorted(search_docs_rank.items(), key=lambda item: item[1], reverse=True))\n",
        "  print(\"Search Results for: \" + search_text)\n",
        "  for i in range(1,6):\n",
        "    # print(str(list(sorted_by_value.keys())[i-1]) + \"\\n\")\n",
        "    # print(str(list(sorted_by_value.values())[i-1]) + \"\\n\")\n",
        "    print(str(i) + \". \" + str(list(sorted_by_value.keys())[i-1]) + \" : \" + str(list(sorted_by_value.values())[i-1]))"
      ],
      "metadata": {
        "id": "4G8w3KAKw_Zn"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_idf_search(\"espresso\")\n",
        "\n",
        "tf_idf_search(\"turkish coffee\")\n",
        "\n",
        "tf_idf_search(\"making a decaffeinated coffee\")\n",
        "\n",
        "tf_idf_search(\"can I use the same coffee grounds twice\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4WrjCxNxW7S",
        "outputId": "edb0afc2-9dec-4dfb-ba51-b42aaeeda386"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search Results for: espresso\n",
            "1. 4404 : 0.3755715977425341\n",
            "2. 3904 : 0.3755715977425341\n",
            "3. 2867 : 0.32191851235074354\n",
            "4. 5526 : 0.307285852698437\n",
            "5. 4258 : 0.30045727819402734\n",
            "Search Results for: turkish coffee\n",
            "1. 5182 : 1.406828518900155\n",
            "2. 5094 : 1.0586643529899669\n",
            "3. 209 : 0.750308543413416\n",
            "4. 483 : 0.750308543413416\n",
            "5. 2522 : 0.6596504998045386\n",
            "Search Results for: making a decaffeinated coffee\n",
            "1. 204 : 1.0611436096253164\n",
            "2. 120 : 0.8098105547792871\n",
            "3. 2897 : 0.7428005267377216\n",
            "4. 3293 : 0.5584774999332578\n",
            "5. 373 : 0.539219655107973\n",
            "Search Results for: can I use the same coffee grounds twice\n",
            "1. 2683 : 0.8569033712400295\n",
            "2. 1749 : 0.5285781531033462\n",
            "3. 3258 : 0.5150073113135012\n",
            "4. 3966 : 0.5093941577180555\n",
            "5. 183 : 0.42714003108048576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VSM"
      ],
      "metadata": {
        "id": "x3agWuAEwUOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_vsm():\n",
        "\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  stop_words.add('p')\n",
        "\n",
        "  # initialize a list\n",
        "  tokens = []\n",
        "\n",
        "  # go thru all questions\n",
        "  for question_id in post_reader.map_questions:\n",
        "    question = post_reader.map_questions[question_id]\n",
        "\n",
        "    # tokenize the title and body and filter out the unwanted stuff\n",
        "    word_tokens = word_tokenize(question.title)\n",
        "    word_tokens += word_tokenize(question.body)\n",
        "    filtered_text = [word.lower() for word in word_tokens if word not in stop_words and word.isalnum()]\n",
        "\n",
        "    # add all of the words into lsit of tokens\n",
        "    for word in filtered_text:\n",
        "      tokens.append(word)\n",
        "    \n",
        "  \n",
        "  # go thru all the answers\n",
        "  for answer_id in post_reader.map_just_answers:\n",
        "    answer = post_reader.map_just_answers[answer_id]\n",
        "\n",
        "    # tokenize the body and filter out the unwanted stuff\n",
        "    word_tokens = word_tokenize(answer.body)\n",
        "    filtered_text = [word.lower() for word in word_tokens if word not in stop_words and word.isalnum()]\n",
        "\n",
        "    # add all of the words into lsit of tokens\n",
        "    for word in filtered_text:\n",
        "      tokens.append(word)\n",
        "\n",
        "  # get the amount of unique tokens\n",
        "  unique_tokens = set(tokens)\n",
        "  unique_count = len(unique_tokens)\n",
        "\n",
        "  # create dictionary of unique terms with an index\n",
        "  i = 0\n",
        "  unique_dict = {}\n",
        "  for word in unique_tokens:\n",
        "    unique_dict[word] = i\n",
        "    i += 1\n",
        "\n",
        "  # initizalize a dictionary for storing the vectors\n",
        "  doc_vectors_dict = {}\n",
        "\n",
        "  # go thru all questions\n",
        "  for question_id in post_reader.map_questions:\n",
        "    question = post_reader.map_questions[question_id]\n",
        "\n",
        "    # tokenize the title and body and filter out the unwanted stuff\n",
        "    word_tokens = word_tokenize(question.title)\n",
        "    word_tokens += word_tokenize(question.body)\n",
        "    filtered_text = [word.lower() for word in word_tokens if word not in stop_words and word.isalnum()]\n",
        "\n",
        "    # go thru all the tokens and create a vector based off of them\n",
        "    temp_vector = [0] * unique_count\n",
        "    for word in filtered_text:\n",
        "      temp_vector[unique_dict[word]] += 1\n",
        "    \n",
        "    # add the vector to the document vectors dictionary\n",
        "    doc_vectors_dict[question_id] = temp_vector.copy()\n",
        "  \n",
        "  # go thru all answers\n",
        "  for answer_id in post_reader.map_just_answers:\n",
        "    answer = post_reader.map_just_answers[answer_id]\n",
        "\n",
        "    # tokenize the title and body and filter out the unwanted stuff\n",
        "    word_tokens = word_tokenize(answer.body)\n",
        "    filtered_text = [word.lower() for word in word_tokens if word not in stop_words and word.isalnum()]\n",
        "\n",
        "    # go thru all the tokens and create a vector based off of them\n",
        "    temp_vector = [0] * unique_count\n",
        "    for word in filtered_text:\n",
        "      temp_vector[unique_dict[word]] += 1\n",
        "    \n",
        "    # add the vector to the document vectors dictionary\n",
        "    doc_vectors_dict[answer_id] = temp_vector.copy()\n",
        "  \n",
        "  # return the document vectors dictionary, unique words, and number of unique words\n",
        "  return doc_vectors_dict, unique_dict, unique_count\n",
        "\n",
        "doc_vectors_dict, unique_dict, unique_count = process_vsm()"
      ],
      "metadata": {
        "id": "gf3SgIgdwl0X"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vsm_search(search_text):\n",
        "\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  stop_words.add('p')\n",
        "\n",
        "  # tokenize the body and filter out the unwanted stuff\n",
        "  word_tokens = word_tokenize(search_text)\n",
        "  search_words = [word.lower() for word in word_tokens if word not in stop_words and word.isalnum()]\n",
        "\n",
        "  # create a vector based off the search\n",
        "  search_vector = [0] * unique_count\n",
        "  for word in search_words:\n",
        "    search_vector[unique_dict[word]] += 1\n",
        "  \n",
        "  # create a dictionary for the scores of documents\n",
        "  docs_scored = {}\n",
        "  for doc_id in doc_vectors_dict.keys():\n",
        "    docs_scored[doc_id] = 1 - distance.cosine(search_vector, doc_vectors_dict[doc_id])\n",
        "  \n",
        "  # print(search_docs_rank)\n",
        "  # order by the values\n",
        "  sorted_by_value = dict(sorted(docs_scored.items(), key=lambda item: item[1], reverse=True))\n",
        "  print(\"Search Results for: \" + search_text)\n",
        "  for i in range(1,6):\n",
        "    # print(str(list(sorted_by_value.keys())[i-1]) + \"\\n\")\n",
        "    # print(str(list(sorted_by_value.values())[i-1]) + \"\\n\")\n",
        "    print(str(i) + \". \" + str(list(sorted_by_value.keys())[i-1]) + \" : \" + str(list(sorted_by_value.values())[i-1]))"
      ],
      "metadata": {
        "id": "npTqatt14C6X"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vsm_search(\"espresso\")\n",
        "\n",
        "vsm_search(\"turkish coffee\")\n",
        "\n",
        "vsm_search(\"making a decaffeinated coffee\")\n",
        "\n",
        "vsm_search(\"can I use the same coffee grounds twice\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nd-ph-ya6w0g",
        "outputId": "32806cb9-243f-496e-d673-f16f3d5de1d7"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search Results for: espresso\n",
            "1. 26 : 0.6108472217815261\n",
            "2. 3956 : 0.5960395606792697\n",
            "3. 2095 : 0.5790416107014432\n",
            "4. 2766 : 0.5746957711326909\n",
            "5. 1574 : 0.5669084372142384\n",
            "Search Results for: turkish coffee\n",
            "1. 5094 : 0.7372097807744856\n",
            "2. 3074 : 0.7219948723811555\n",
            "3. 5182 : 0.7071067811865476\n",
            "4. 2379 : 0.6616825781270739\n",
            "5. 5095 : 0.629511580291707\n",
            "Search Results for: making a decaffeinated coffee\n",
            "1. 120 : 0.5715476066494082\n",
            "2. 3746 : 0.5101127853361852\n",
            "3. 3509 : 0.48666426339228763\n",
            "4. 2158 : 0.4838867031273071\n",
            "5. 373 : 0.4758309514308865\n",
            "Search Results for: can I use the same coffee grounds twice\n",
            "1. 1749 : 0.6757246285173464\n",
            "2. 2683 : 0.6459422414661738\n",
            "3. 3258 : 0.6383942119187179\n",
            "4. 4959 : 0.5976415302112609\n",
            "5. 3144 : 0.5952522654434597\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BM25"
      ],
      "metadata": {
        "id": "bh_uoIg-82rj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_bm25():\n",
        "\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  stop_words.add('p')\n",
        "\n",
        "  # initialize tf, idf, document length dictionaries, total docs, and total tokens\n",
        "  tf_dict = {}\n",
        "  idf_dict = {}\n",
        "  doc_length = {}\n",
        "  total_docs = 0\n",
        "  total_tokens = 0\n",
        "\n",
        "  # calculate tf\n",
        "  # go thru all questions\n",
        "  for question_id in post_reader.map_questions:\n",
        "    question = post_reader.map_questions[question_id]\n",
        "\n",
        "    # tokenize the title and body and filter out the unwanted stuff\n",
        "    word_tokens = word_tokenize(question.title)\n",
        "    word_tokens += word_tokenize(question.body)\n",
        "    filtered_text = [word.lower() for word in word_tokens if word not in stop_words and word.isalnum()]\n",
        "\n",
        "    # get the amount of terms in the doc\n",
        "    terms_amt = len(filtered_text)\n",
        "\n",
        "    # add doc length to dict, increment total docs and total tokens\n",
        "    doc_length[question_id] = terms_amt\n",
        "    total_docs += 1\n",
        "    total_tokens += terms_amt\n",
        "\n",
        "    # go thru all the strings in filtered_text\n",
        "    for word in filtered_text:\n",
        "      # if the word is already in the dictionary\n",
        "      if word in tf_dict.keys():\n",
        "        # if the questionid for the word is there, increment\n",
        "        if question_id in tf_dict[word].keys():\n",
        "          tf_dict[word][question_id] += (1 / terms_amt)\n",
        "\n",
        "        # if the questionid for the word is not there, initialize\n",
        "        else:\n",
        "          tf_dict[word][question_id] = (1 / terms_amt)\n",
        "\n",
        "      # if the word is not in the dictionary, initialize it\n",
        "      else:\n",
        "        tf_dict[word] = {question_id: (1 / terms_amt)}\n",
        "\n",
        "  # go thru all the answers\n",
        "  for answer_id in post_reader.map_just_answers:\n",
        "    answer = post_reader.map_just_answers[answer_id]\n",
        "\n",
        "    # tokenize the body and filter out the unwanted stuff\n",
        "    word_tokens = word_tokenize(answer.body)\n",
        "    filtered_text = [word.lower() for word in word_tokens if word not in stop_words and word.isalnum()]\n",
        "\n",
        "    # get the amount of terms in the doc\n",
        "    terms_amt = len(filtered_text)\n",
        "\n",
        "    # add doc length to dict, increment total docs and total tokens\n",
        "    doc_length[answer_id] = terms_amt\n",
        "    total_docs += 1\n",
        "    total_tokens += terms_amt\n",
        "\n",
        "    # go thru all the strings in filtered_text\n",
        "    for word in filtered_text:\n",
        "      # if the word is already in the dictionary\n",
        "      if word in tf_dict.keys():\n",
        "        # if the answerid for the word is there, increment\n",
        "        if answer_id in tf_dict[word].keys():\n",
        "          tf_dict[word][answer_id] += (1 / terms_amt)\n",
        "\n",
        "        # if the answerid for the word is not there, initialize\n",
        "        else:\n",
        "          tf_dict[word][answer_id] = (1 / terms_amt)\n",
        "\n",
        "      # if the word is not in the dictionary, initialize it\n",
        "      else:\n",
        "        tf_dict[word] = {answer_id: (1 / terms_amt)}\n",
        "    \n",
        "  \n",
        "  # calculate idf\n",
        "  # get total amount of docs\n",
        "  doc_amount = len(post_reader.map_questions) + len(post_reader.map_just_answers)\n",
        "\n",
        "  # go thru all the words in tf dictionary and calculate idf\n",
        "  idf_dict = {word: (math.log( (doc_amount / len(tf_dict[word].keys())) , 2)) for word in tf_dict.keys()}\n",
        "\n",
        "  # return tf, idf, document length dictionaries and average length\n",
        "  return tf_dict, idf_dict, doc_length, total_tokens / total_docs\n",
        "\n",
        "# Process BM25\n",
        "tf_dict, idf_dict, doc_length, average_length = process_bm25()"
      ],
      "metadata": {
        "id": "WGKxjIPJ87ux"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a search and print the top results\n",
        "def bm_25_search(search_text, b, k1):\n",
        "\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  stop_words.add('p')\n",
        "\n",
        "  # tokenize the body and filter out the unwanted stuff\n",
        "  word_tokens = word_tokenize(search_text)\n",
        "  search_words = [word.lower() for word in word_tokens if word not in stop_words and word.isalnum()]\n",
        "\n",
        "  # initialize a dictionary to store document scores\n",
        "  doc_scores = {}\n",
        "\n",
        "  # go thru all the terms\n",
        "  for word in search_words:\n",
        "    # go thru all the docs for the word\n",
        "    for doc_id in tf_dict[word].keys():\n",
        "      # if the doc has not been score, initialize\n",
        "      if doc_id not in doc_scores.keys():\n",
        "        doc_scores[doc_id] = (idf_dict[word] * (((k1 + 1) * tf_dict[word][doc_id]) / ((k1 * ((1 - b) + (b * (doc_length[doc_id] / average_length)))) + tf_dict[word][doc_id])))\n",
        "      # if the doc has been scored, increment\n",
        "      else:\n",
        "        doc_scores[doc_id] += (idf_dict[word] * (((k1 + 1) * tf_dict[word][doc_id]) / ((k1 * ((1 - b) + (b * (doc_length[doc_id] / average_length)))) + tf_dict[word][doc_id])))\n",
        "\n",
        "  \n",
        "  # print(search_docs_rank)\n",
        "  # order by the values\n",
        "  sorted_by_value = dict(sorted(doc_scores.items(), key=lambda item: item[1], reverse=True))\n",
        "  print(\"Search Results for: \" + search_text)\n",
        "  for i in range(1,6):\n",
        "    # print(str(list(sorted_by_value.keys())[i-1]) + \"\\n\")\n",
        "    # print(str(list(sorted_by_value.values())[i-1]) + \"\\n\")\n",
        "    print(str(i) + \". \" + str(list(sorted_by_value.keys())[i-1]) + \" : \" + str(list(sorted_by_value.values())[i-1]))"
      ],
      "metadata": {
        "id": "a14fozitFDwJ"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b = 0.75\n",
        "k1 = 1.2\n",
        "\n",
        "bm_25_search(\"espresso\", b, k1)\n",
        "\n",
        "bm_25_search(\"turkish coffee\", b, k1)\n",
        "\n",
        "bm_25_search(\"making a decaffeinated coffee\", b, k1)\n",
        "\n",
        "bm_25_search(\"can I use the same coffee grounds twice\", b, k1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYOOVH9JIzDW",
        "outputId": "00461141-2aa8-49e8-d403-ff7b9818c380"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search Results for: espresso\n",
            "1. 3904 : 1.547084428490502\n",
            "2. 4404 : 1.3737044945367798\n",
            "3. 2867 : 1.0433688725826213\n",
            "4. 5526 : 0.9890349752828139\n",
            "5. 3981 : 0.96761757755849\n",
            "Search Results for: turkish coffee\n",
            "1. 5182 : 5.202259238172734\n",
            "2. 5094 : 3.796604360991974\n",
            "3. 209 : 2.742667548141526\n",
            "4. 483 : 2.742667548141526\n",
            "5. 2522 : 2.0765703993535936\n",
            "Search Results for: making a decaffeinated coffee\n",
            "1. 204 : 3.8899468072956545\n",
            "2. 2897 : 2.6159512025630143\n",
            "3. 120 : 2.460535411591724\n",
            "4. 3293 : 2.296214284092522\n",
            "5. 373 : 1.6920452949896814\n",
            "Search Results for: can I use the same coffee grounds twice\n",
            "1. 2683 : 2.47962822849626\n",
            "2. 3966 : 2.0091653855787985\n",
            "3. 1749 : 1.6049621533294713\n",
            "4. 3818 : 1.5168800892726597\n",
            "5. 4703 : 1.4473461318125382\n"
          ]
        }
      ]
    }
  ]
}